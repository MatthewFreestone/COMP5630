{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                                \n",
    "import numpy as np       \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>Targets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-10.000000</td>\n",
       "      <td>-12.494938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-9.959920</td>\n",
       "      <td>-16.144029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-9.919840</td>\n",
       "      <td>-16.175059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-9.879760</td>\n",
       "      <td>-23.393130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-9.839679</td>\n",
       "      <td>-12.531222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Features    Targets\n",
       "0 -10.000000 -12.494938\n",
       "1  -9.959920 -16.144029\n",
       "2  -9.919840 -16.175059\n",
       "3  -9.879760 -23.393130\n",
       "4  -9.839679 -12.531222"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"numpydataset.csv\")      # load the csv file as a DataFrame\n",
    "data.head()                                 # displays the first 5 rows in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = len(data)              # calculating number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(points, m, b):\n",
    "    # write your code here...\n",
    "    x = points[\"Features\"]\n",
    "    y = points[\"Targets\"]\n",
    "    pred = m * x + b\n",
    "    return np.mean((y-pred)**2)\n",
    "\n",
    "\n",
    "def gradient_descent(m_current, b_current, points, step_size):\n",
    "    x = points[\"Features\"]\n",
    "    y = points[\"Targets\"]\n",
    "    n = len(x)\n",
    "    pred = m_current * x + b_current\n",
    "\n",
    "    b = b_current - step_size * ((-2/n) * np.sum(y - pred))\n",
    "    m = m_current - step_size * ((-2/n) *  np.dot(x, y - pred))\n",
    "    return m,b \n",
    "    # write your code here...\n",
    "    \n",
    "    #return m_new, b_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, m: 0.13658349190618496, b:0.010312358814106724, Loss: 164.80508456949963\n",
      "Epoch 2, m: 0.26402492243027254, b:0.02060409291058523, Loss: 149.00149960714552\n",
      "Epoch 3, m: 0.38293620509964166, b:0.03087524353887078, Loss: 135.22943429119377\n",
      "Epoch 4, m: 0.493888295702564, b:0.04112585186589976, Loss: 123.2260882044345\n",
      "Epoch 5, m: 0.5974139337480964, b:0.05135595897627468, Loss: 112.76266383116672\n",
      "Epoch 6, m: 0.6940102004294588, b:0.061565605872428854, Loss: 103.63996700446963\n",
      "Epoch 7, m: 0.7841409053730307, b:0.07175483347479072, Loss: 95.68457660100778\n",
      "Epoch 8, m: 0.8682388136330048, b:0.08192368262194787, Loss: 88.74550982980173\n",
      "Epoch 9, m: 0.946707723624676, b:0.09207219407081069, Loss: 82.69131899125422\n",
      "Epoch 10, m: 1.019924405973618, b:0.10220040849677578, Loss: 77.40756387953365\n",
      "Epoch 11, m: 1.0882404125901861, b:0.11230836649388895, Loss: 72.7946112247259\n",
      "Epoch 12, m: 1.1519837646556652, b:0.12239610857500789, Loss: 68.76571885986522\n",
      "Epoch 13, m: 1.211460527624978, b:0.1324636751719646, Loss: 65.24536777297834\n",
      "Epoch 14, m: 1.2669562808083688, b:0.1425111066357274, Loss: 62.1678099708959\n",
      "Epoch 15, m: 1.318737488588302, b:0.15253844323656268, Loss: 59.47580423146678\n",
      "Epoch 16, m: 1.3670527798555105, b:0.16254572516419627, Loss: 57.11951543374669\n",
      "Epoch 17, m: 1.412134141807439, b:0.1725329925279746, Loss: 55.055556301200994\n",
      "Epoch 18, m: 1.454198033841142, b:0.18250028535702537, Loss: 53.24615313144459\n",
      "Epoch 19, m: 1.493446426889022, b:0.19244764360041805, Loss: 51.65841947019919\n",
      "Epoch 20, m: 1.5300677731878054, b:0.20237510712732393, Loss: 50.26372376282579\n",
      "Epoch 21, m: 1.5642379111371312, b:0.212282715727176, Loss: 49.03713882389985\n",
      "Epoch 22, m: 1.596120909592454, b:0.22217050910982838, Loss: 47.95696253858989\n",
      "Epoch 23, m: 1.6258698556461582, b:0.23203852690571544, Loss: 47.00430057932758\n",
      "Epoch 24, m: 1.653627589679434, b:0.24188680866601073, Loss: 46.16270311376041\n",
      "Epoch 25, m: 1.67952739121429, b:0.2517153938627854, Loss: 45.41784851818469\n",
      "Epoch 26, m: 1.7036936188588372, b:0.2615243218891666, Loss: 44.75726801453283\n",
      "Epoch 27, m: 1.7262423074185589, b:0.271313632059495, Loss: 44.17010593591505\n",
      "Epoch 28, m: 1.7472817250406159, b:0.2810833636094827, Loss: 43.64691101082228\n",
      "Epoch 29, m: 1.7669128930663267, b:0.2908335556963705, Loss: 43.17945465256066\n",
      "Epoch 30, m: 1.7852300710879119, b:0.3005642473990845, Loss: 42.76057275977557\n",
      "Epoch 31, m: 1.8023212095385133, b:0.31027547771839303, Loss: 42.38402798602196\n",
      "Epoch 32, m: 1.8182683719886135, b:0.319967285577063, Loss: 42.0443898299404\n",
      "Epoch 33, m: 1.8331481291765226, b:0.3296397098200156, Loss: 41.73693024027347\n",
      "Epoch 34, m: 1.8470319266648803, b:0.3392927892144823, Loss: 41.45753272829492\n",
      "Epoch 35, m: 1.8599864278884861, b:0.34892656245016007, Loss: 41.202613239959604\n",
      "Epoch 36, m: 1.872073834240612, b:0.3585410681393665, Loss: 40.96905126621258\n",
      "Epoch 37, m: 1.8833521837346998, b:0.3681363448171945, Loss: 40.75412986676654\n",
      "Epoch 38, m: 1.893875629675476, b:0.37771243094166684, Loss: 40.555483454055086\n",
      "Epoch 39, m: 1.9036947006775309, b:0.38726936489389024, Loss: 40.37105233329189\n",
      "Epoch 40, m: 1.912856543279849, b:0.3968071849782092, Loss: 40.199043124479026\n",
      "Epoch 41, m: 1.9214051483212105, b:0.4063259294223595, Loss: 40.037894305313465\n",
      "Epoch 42, m: 1.9293815621634105, b:0.41582563637762154, Loss: 39.88624621241109\n",
      "Epoch 43, m: 1.9368240837764934, b:0.42530634391897304, Loss: 39.74291492399707\n",
      "Epoch 44, m: 1.943768448632308, b:0.4347680900452418, Loss: 39.606869521849205\n",
      "Epoch 45, m: 1.9502480002893565, b:0.44421091267925805, Loss: 39.47721229526082\n",
      "Epoch 46, m: 1.956293850492807, b:0.45363484966800627, Loss: 39.353161506362255\n",
      "Epoch 47, m: 1.9619350285583912, b:0.463039938782777, Loss: 39.23403638539313\n",
      "Epoch 48, m: 1.9671986207574614, b:0.47242621771931814, Loss: 39.1192440673973\n",
      "Epoch 49, m: 1.9721099003724656, b:0.4817937240979862, Loss: 39.00826821914453\n",
      "Epoch 50, m: 1.9766924490473072, b:0.49114249546389693, Loss: 38.9006591375846\n",
      "Epoch 51, m: 1.9809682700152556, b:0.5004725692870758, Loss: 38.796025129435684\n",
      "Epoch 52, m: 1.9849578937480747, b:0.5097839829626084, Loss: 38.69402500614435\n",
      "Epoch 53, m: 1.988680476533647, b:0.5190767738107899, Loss: 38.59436154990157\n",
      "Epoch 54, m: 1.9921538924554154, b:0.528350979077275, Loss: 38.496775825072696\n",
      "Epoch 55, m: 1.995394819215286, b:0.5376066359332272, Loss: 38.40104222565473\n",
      "Epoch 56, m: 1.998418818212071, b:0.5468437814754675, Loss: 38.30696416352858\n",
      "Epoch 57, m: 2.001240409259973, b:0.5560624527266234, Loss: 38.214370314595214\n",
      "Epoch 58, m: 2.0038731403058714, b:0.5652626866352768, Loss: 38.12311135061229\n",
      "Epoch 59, m: 2.00632965248016, b:0.5744445200761129, Loss: 38.03305709388804\n",
      "Epoch 60, m: 2.0086217407934845, b:0.5836079898500675, Loss: 37.944094040119595\n",
      "Epoch 61, m: 2.010760410770807, b:0.592753132684474, Loss: 37.85612320174256\n",
      "Epoch 62, m: 2.0127559312947376, b:0.6018799852332118, Loss: 37.76905823032188\n",
      "Epoch 63, m: 2.014617883911856, b:0.6109885840768522, Loss: 37.68282378187898\n",
      "Epoch 64, m: 2.0163552088387706, b:0.6200789657228052, Loss: 37.59735409372296\n",
      "Epoch 65, m: 2.0179762478888135, b:0.6291511666054663, Loss: 37.51259174541924\n",
      "Epoch 66, m: 2.0194887845254867, b:0.6382052230863621, Loss: 37.42848658007095\n",
      "Epoch 67, m: 2.0209000812349758, b:0.647241171454296, Loss: 37.34499476517041\n",
      "Epoch 68, m: 2.0222169143971764, b:0.6562590479254942, Loss: 37.26207797496211\n",
      "Epoch 69, m: 2.0234456068226687, b:0.6652588886437499, Loss: 37.17970267859525\n",
      "Epoch 70, m: 2.0245920581118653, b:0.6742407296805691, Loss: 37.09783952037769\n",
      "Epoch 71, m: 2.025661772982106, b:0.6832046070353147, Loss: 37.016462780214916\n",
      "Epoch 72, m: 2.026659887698707, b:0.6921505566353509, Loss: 36.935549903858785\n",
      "Epoch 73, m: 2.0275911947368823, b:0.7010786143361869, Loss: 36.85508109393373\n",
      "Epoch 74, m: 2.0284601657929433, b:0.7099888159216212, Loss: 36.7750389538764\n",
      "Epoch 75, m: 2.029270973255272, b:0.7188811971038847, Loss: 36.69540817794271\n",
      "Epoch 76, m: 2.030027510238158, b:0.7277557935237836, Loss: 36.61617528132135\n",
      "Epoch 77, m: 2.0307334092746947, b:0.7366126407508428, Loss: 36.537328365164825\n",
      "Epoch 78, m: 2.031392059758485, b:0.7454517742834478, Loss: 36.45885691202005\n",
      "Epoch 79, m: 2.03200662421791, b:0.7542732295489876, Loss: 36.38075160772502\n",
      "Epoch 80, m: 2.0325800535010923, b:0.7630770419039963, Loss: 36.30300418634743\n",
      "Epoch 81, m: 2.0331151009444786, b:0.771863246634295, Loss: 36.22560729518375\n",
      "Epoch 82, m: 2.033614335593057, b:0.7806318789551331, Loss: 36.14855437722324\n",
      "Epoch 83, m: 2.034080154535699, b:0.7893829740113295, Loss: 36.07183956881722\n",
      "Epoch 84, m: 2.034514794414845, b:0.7981165668774136, Loss: 35.99545761058604\n",
      "Epoch 85, m: 2.034920342165808, b:0.8068326925577655, Loss: 35.919403769851336\n",
      "Epoch 86, m: 2.0352987450372475, b:0.8155313859867567, Loss: 35.843673773101884\n",
      "Epoch 87, m: 2.0356518199409392, b:0.8242126820288899, Loss: 35.768263747195235\n",
      "Epoch 88, m: 2.0359812621757265, b:0.8328766154789388, Loss: 35.693170168164606\n",
      "Epoch 89, m: 2.0362886535675444, b:0.8415232210620877, Loss: 35.61838981664719\n",
      "Epoch 90, m: 2.0365754700645993, b:0.8501525334340702, Loss: 35.54391973907687\n",
      "Epoch 91, m: 2.0368430888241758, b:0.8587645871813088, Loss: 35.46975721389594\n",
      "Epoch 92, m: 2.0370927948250954, b:0.8673594168210529, Loss: 35.39589972213599\n",
      "Epoch 93, m: 2.0373257870375765, b:0.8759370568015175, Loss: 35.32234492180301\n",
      "Epoch 94, m: 2.037543184180124, b:0.8844975415020211, Loss: 35.249090625574254\n",
      "Epoch 95, m: 2.0377460300910863, b:0.8930409052331237, Loss: 35.17613478137856\n",
      "Epoch 96, m: 2.0379352987406736, b:0.9015671822367642, Loss: 35.10347545548689\n",
      "Epoch 97, m: 2.038111898907503, b:0.9100764066863974, Loss: 35.031110817788445\n",
      "Epoch 98, m: 2.0382766785421236, b:0.9185686126871314, Loss: 34.95903912896947\n",
      "Epoch 99, m: 2.038430428838475, b:0.9270438342758639, Loss: 34.8872587293487\n",
      "Epoch 100, m: 2.038573888032826, b:0.9355021054214189, Loss: 34.81576802915489\n",
      "2.038573888032826 0.9355021054214189 34.81576802915489\n"
     ]
    }
   ],
   "source": [
    "m, b = 0, 0\n",
    "L = 0.001       # initial learning rate, can be adjusted later\n",
    "epochs = 100    # we iterate over the same dataset 100 times\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    m, b = gradient_descent(m, b, data, L)\n",
    "    loss = MSE(data, m, b)\n",
    "    print(f\"Epoch {epoch}, m: {m}, b:{b}, Loss: {loss}\")\n",
    "print(m, b, loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "\n",
    "ax.scatter(data.Features, \n",
    "           data.Targets, \n",
    "           color=\"red\", \n",
    "           linewidths=0.5, \n",
    "           label=\"Points\")\n",
    "ax.plot(data.Features, \n",
    "        [m * x + b for x in data.Features], \n",
    "        linewidth=3, \n",
    "        linestyle=\"dashed\", \n",
    "        label=\"$ f(x) = mx+c $\")\n",
    "\n",
    "ax.legend(loc=\"lower right\", bbox_to_anchor=(.96, 0.0))\n",
    "ax.set_xlabel(\"Features\")\n",
    "ax.set_ylabel(\"Targets\")\n",
    "\n",
    "plt.savefig('LinearRegression001.png')\n",
    "\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "m, b = 0, 0\n",
    "L = 0.01   # new learning rate\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    m, b = gradient_descent(m, b, data, L)\n",
    "    loss = MSE(data, m, b)\n",
    "    print(f\"Epoch {epoch}, m: {m}, b:{b}, Loss: {loss}\")\n",
    "print(m, b, loss)\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "\n",
    "ax.scatter(data.Features, \n",
    "           data.Targets, \n",
    "           color=\"red\", \n",
    "           linewidths=0.5, \n",
    "           label=\"Points\")\n",
    "ax.plot(data.Features, \n",
    "        [m * x + b for x in data.Features], \n",
    "        linewidth=3, \n",
    "        linestyle=\"dashed\", \n",
    "        label=\"$ f(x) = mx+c $\")\n",
    "\n",
    "ax.legend(loc=\"lower right\", bbox_to_anchor=(.96, 0.0))\n",
    "ax.set_xlabel(\"Features\")\n",
    "ax.set_ylabel(\"Targets\")\n",
    "\n",
    "plt.savefig('LinearRegression01.png')\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fc991c51327496ebdbfda1f0a33f704ad01b3cb47eab69be37a6eb36834cd500"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
